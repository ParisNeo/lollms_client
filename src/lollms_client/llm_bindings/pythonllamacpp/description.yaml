title: Python Llama.cpp
author: ParisNeo
creation_date: 2025-08-13
last_update_date: 2025-08-13
description: A direct, in-process binding using the `llama-cpp-python` library to run GGUF models locally. This offers high performance by avoiding network overhead.
input_parameters:
  - name: model_path
    type: str
    description: "The full path to the GGUF model file you want to load."
    mandatory: true
    default: ""
  - name: n_gpu_layers
    type: int
    description: "The number of model layers to offload to the GPU. Set to -1 for all, 0 for none. Defaults to 0."
    mandatory: false
    default: 0
  - name: n_ctx
    type: int
    description: "The context window size in tokens for the model. Defaults to 2048."
    mandatory: false
    default: 2048
  - name: seed
    type: int
    description: "The random seed for generation. Set to -1 for random. Defaults to -1."
    mandatory: false
    default: -1
  - name: chat_format
    type: str
    description: "The chat format template to use (e.g., 'chatml', 'llava-1-5')."
    mandatory: false
    default: "chatml"
  - name: clip_model_path
    type: str
    description: "The path to the mmproj file for LLaVA vision support."
    mandatory: false
    default: ""