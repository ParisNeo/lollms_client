title: Llama.cpp Server
author: ParisNeo
creation_date: 2025-08-13
last_update_date: 2025-08-13
description: Manages and communicates with a locally-run llama.cpp server process for running GGUF models with high performance. Supports multi-user access and vision models (LLaVA).
input_parameters:
  - name: models_path
    type: str
    description: "The path to the directory where your GGUF model files are stored."
    mandatory: true
    default: ""
  - name: model_name
    type: str
    description: "The filename of the GGUF model to load (e.g., 'Llama-3-8B-Instruct.Q4_K_M.gguf'). If not provided, the binding will try to auto-select one."
    mandatory: false
    default: ""
  - name: clip_model_name
    type: str
    description: "The filename of the LLaVA clip model (mmproj file) for vision support. If not provided, it will be auto-detected based on the main model's name."
    mandatory: false
    default: ""
  - name: n_gpu_layers
    type: int
    description: "The number of model layers to offload to the GPU. Set to -1 to offload all layers, 0 for CPU only. Defaults to 0."
    mandatory: false
    default: 0
  - name: n_ctx
    type: int
    description: "The context window size in tokens for the model. Defaults to 128000."
    mandatory: false
    default: 128000
  - name: seed
    type: int
    description: "The random seed for generation. Set to -1 for a random seed. Defaults to -1."
    mandatory: false
    default: -1